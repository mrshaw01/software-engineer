# LLM Evaluation Metrics

A curated collection of evaluation metrics for Large Language Models (LLMs), organized by category.
Each metric has its own folder with a dedicated `README.md` explaining its definition, use cases, pros/cons, and example code.

## Categories

- **Text Quality (Reference-Based)** — BLEU, ROUGE, METEOR, BERTScore, CHRF, MoverScore, BLEURT
- **Semantic Similarity** — Cosine Similarity, USE Score, LLM-as-a-Judge
- **Task-Specific** — Exact Match, F1 Score, Accuracy, Perplexity
- **Human Evaluation** — Fluency, Coherence, Relevance, Faithfulness, Helpfulness, Safety
- **Robustness, Bias & Safety** — Toxicity Scores, Bias Probes, TruthfulnessQA, Adversarial Robustness

## Purpose

This repository serves as a quick reference for understanding, implementing, and comparing LLM evaluation metrics in research and production.
