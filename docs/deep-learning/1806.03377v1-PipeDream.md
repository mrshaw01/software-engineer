### PipeDream Summary (Engineering Design Doc)

**PipeDream** is a distributed DNN training system that combines **model parallelism, data parallelism, and pipeline parallelism** to reduce communication overhead and improve time-to-accuracy for large models.

#### **Key Concepts**

- **Pipeline Parallelism:** Partitions DNN layers into stages, assigning each stage to a GPU. Multiple mini-batches are injected to keep all GPUs busy simultaneously.
- **Hybrid Parallelism:** Some stages are replicated (data parallelism) while others remain pipelined, balancing compute and minimizing communication.
- **Automatic Partitioning:** A profiling run estimates per-layer compute/communication cost. A dynamic programming algorithm determines optimal stage boundaries and replication factors.
- **Scheduling (1F1B):** Alternates forward and backward passes per GPU in steady state, maximizing utilization and maintaining training progress.
- **Weight Stashing:** Maintains per-mini-batch weight versions to ensure forward and backward passes use consistent parameters, preventing convergence issues.

#### **Advantages**

- **Up to 95% reduction in communication** compared to data-parallel training by transferring only activations/gradients between stages.
- **Faster time-to-accuracy:** Achieves up to **5Ã— speedup** over BSP data-parallel training for models like VGG16 and AlexNet.
- **Fully automated partitioning** removes the need for manual device placement.

#### **Design Highlights**

| Component         | Description                                                                      |
| ----------------- | -------------------------------------------------------------------------------- |
| **Profiler**      | Measures per-layer compute time, output size, and parameter size.                |
| **Optimizer**     | Uses DP to partition layers, assign replication factors, and set pipeline depth. |
| **Stage Runtime** | Manages GPU memory, inter-stage communication, scheduling, and weight stashing.  |

#### **Impact**

PipeDream laid the groundwork for **pipeline parallelism in modern LLM training frameworks** (e.g., Megatron-LM, DeepSpeed), especially for bandwidth-constrained environments where pure data-parallel scaling is inefficient.
