### Comprehensive Report on _DeepCompile: A Compiler-Driven Approach to Optimizing Distributed Deep Learning Training_

#### **1. Introduction**

The rapid growth of large-scale deep learning models has necessitated distributed training across multiple GPUs. Existing fully sharded approaches such as **DeepSpeed ZeRO-3** and **PyTorch FSDP** split parameters across GPUs and gather them as needed. These frameworks employ **prefetching**, **unsharding**, and **offloading** to improve performance, but suffer from **limited flexibility** in adapting to dynamic memory usage and coordinating multiple optimizations effectively.

The paper introduces **DeepCompile**, a compiler-driven system that transforms user-defined models into computation graphs and applies **profiling-guided optimization passes** to improve training efficiency by dynamically managing communication, computation, and memory.

#### **2. Key Contributions**

DeepCompile provides the following advancements over existing frameworks:

1. **Compiler-Based Graph Transformations**

   - Converts models into computation graphs and programmatically inserts distributed training operators (e.g., `all-gather`, `release`).
   - Enables **global, graph-level analysis** rather than relying on runtime hooks.

2. **Profiling-Guided Optimization Passes**

   - Applies a sequence of passes based on runtime memory and execution profiles.
   - Each pass rewrites the computation graph, and subsequent passes adapt to changes made earlier.

3. **Three Core Optimizations:**

   - **Proactive Prefetching:** Dynamically schedules `all-gather` operations earlier when memory allows, improving communication-computation overlap.
   - **Selective Unsharding:** Keeps some parameters unsharded across passes to reduce communication overhead, especially beneficial with gradient accumulation.
   - **Adaptive Offloading:** Offloads only necessary optimizer state fragments to CPU memory and overlaps transfers with computation to reduce overhead.

#### **3. System Design**

- **Workflow:**

  1. A base compiler (e.g., PyTorch compiler) lowers the model to an intermediate representation (IR).
  2. DeepCompile inserts communication operators and applies optimizations.
  3. Profiling information (execution time, communication overhead, memory trends) is gathered after each pass.
  4. The process iterates, progressively refining the computation graph.

- **Advantages over Existing Systems:**

  - Does not require manual model modifications or static heuristics.
  - Flexibly adapts to dynamic memory usage patterns during forward and backward passes.
  - Supports coordinated application of multiple optimizations.

#### **4. Optimizations**

##### **4.1 Fully-Sharded Approach**

- Each parameter tensor is partitioned across GPUs.
- DeepCompile schedules **`all-gather`** operations just before the first use of a parameter and **`release`** operations after its last use, minimizing buffer lifetimes.

##### **4.2 Proactive Prefetching**

- Uses **profiling data** to determine when to initiate `all-gather` earlier without exceeding memory limits.
- Fuses multiple `all-gather` operations when beneficial to reduce communication overhead.

##### **4.3 Selective Unsharding**

- Retains parameters in unsharded form when memory allows, reducing redundant `all-gather` operations across gradient accumulation steps.

##### **4.4 Adaptive Offloading**

- Monitors memory usage in real time and offloads **only the necessary fragments** of optimizer states to CPU memory.
- Transfers are overlapped with computation to hide latency.

#### **5. Evaluation**

##### **Experimental Setup**

- Tested on clusters with **NVIDIA H100 GPUs** and **Llama-3 70B** and **Mixtral 8×7B MoE** models.
- Compared against **ZeRO-3**, **FSDP**, and their versions with PyTorch compilation enabled.

##### **Results**

1. **Efficiency Gains:**

   - DeepCompile achieved **1.28× speedup (Llama-3)** and **1.54× speedup (Mixtral)** over ZeRO-3.
   - Benefits were greater for larger batch sizes and gradient accumulation steps.

2. **Memory Utilization:**

   - Selective unsharding increased GPU memory usage up to **65GB per GPU**, effectively utilizing available resources.

3. **Adaptive Offloading:**

   - Achieved up to **7.01× throughput improvement** compared to ZeRO-3 offloading by overlapping data transfers with computation.

4. **Compilation Time:**

   - One-time compilation overhead ranged between **\~250s to 437s**, negligible relative to total training time.
   - Results can be cached and reused across training runs.

5. **Correctness:**

   - Loss curves closely matched ZeRO-3 baselines, confirming correctness of optimizations.

#### **6. Comparison with Prior Work**

- **ZeRO-3/FSDP:** Rely on runtime hooks and static prefetch buffer sizes. Cannot dynamically adapt or coordinate multiple optimizations.
- **SimpleFSDP:** Optimizes prefetching using compilation but lacks support for unsharding and adaptive offloading.
- **Alpa/FlexFlow/Unity:** Focus on static parallelization planning, not runtime dynamic optimizations.

DeepCompile **combines compiler-based graph transformations with profiling-guided, runtime-aware optimizations**, making it more flexible and extensible.

#### **7. Conclusion**

DeepCompile demonstrates that compiler-based approaches can outperform traditional heuristic-driven frameworks for distributed training by enabling **dynamic memory-aware scheduling** and **coordinated optimizations**. It achieves **higher throughput**, **better memory utilization**, and **greater flexibility** compared to ZeRO-3 and FSDP.

**Future Work:**

- Extending DeepCompile to support **automated parallelization planning**, **dynamic memory scheduling**, and broader optimization strategies.

#### **Key Takeaways**

- Compiler-driven approaches are superior to static runtime hooks for optimizing distributed deep learning training.
- Profiling-guided passes enable **dynamic adaptation** to memory and computation patterns.
- DeepCompile’s improvements are particularly significant for **large models, gradient accumulation, and limited GPU scenarios**.
