## **Report on “Efficient Training of Large Language Models on Distributed Infrastructures: A Survey”**

### **1. Introduction and Motivation**

Large Language Models (LLMs) such as GPT, LLaMA, and Gemini have achieved groundbreaking performance in numerous tasks. Their success is driven by the transformer architecture and massive scaling of model parameters and datasets. However, training LLMs requires **tens of thousands of GPUs or AI accelerators over extended periods**, creating significant challenges in **scalability, efficiency, and reliability (SER)**.

The paper surveys recent advances in LLM training infrastructures and distributed systems. It explores innovations in **hardware accelerators, networking, storage, scheduling**, and **parallelization strategies**. The study also covers optimizations in **computation, communication, and memory**, along with **fault tolerance mechanisms** to maintain training stability over weeks or months.

### **2. Background**

- **Transformer-based LLMs** rely on attention mechanisms to process sequences. Variants like **Multi-Query Attention (MQA)**, **Group-Query Attention (GQA)**, and **Mixture-of-Experts (MoE)** have improved efficiency and scalability.

- **LLM workloads differ** from traditional DL tasks due to:

  1. Homogeneous architecture (mostly transformer-based).
  2. Unprecedented scale in parameters, datasets, and training duration.
  3. Specialized software optimizations (e.g., DeepSpeed, Megatron).
  4. Shift to foundation model pretraining with later fine-tuning.

- **Key challenges**:

  - **Scalability**: Efficiently utilizing thousands of GPUs/accelerators.
  - **Efficiency**: Achieving high Model FLOPs Utilization (MFU).
  - **Reliability**: Handling frequent failures during long training runs.

### **3. Training Infrastructure**

The survey discusses multiple components:

#### **3.1 AI Accelerators**

- NVIDIA GPUs (Ampere, Hopper, Blackwell) dominate due to CUDA support and Tensor Cores.
- Other accelerators: AMD GPUs (MI250X), Intel Gaudi, Google TPU, Graphcore IPU, Cerebras CS-2.

#### **3.2 Networking**

- Communication overhead is a **major bottleneck** in scaling.
- **Chip-to-chip interconnects**: PCIe (limited), NVLink, NVSwitch, TPUv4 torus topologies.
- **Node-to-node communication**: GPUDirect-RDMA, InfiniBand (EDR/HDR/NDR), RoCE.
- **Topologies**: Clos (Fat-Tree), Dragonfly+, rail-optimized designs for collective communication.
- **Load balancing & congestion control**: Enhanced-ECMP, packet spraying, HPCC, TIMELY, DCQCN.

#### **3.3 Storage**

- Checkpoint storage: Tectonic (Meta), HDFS (ByteDance), Ceph object storage.
- Training data storage: Lustre, GPFS, BeeGFS, Alluxio, JuiceFS.
- Emphasis on **high-bandwidth checkpointing and caching** to avoid I/O bottlenecks.

#### **3.4 Scheduling**

- **Workload scheduling**: Crius, Hydro, Acme—optimize GPU allocation, hyperparameter tuning, and failure recovery.
- **Resource scheduling**: Cassini, HIRE, SiloD, Synergy—manage bandwidth, memory, CPU, and energy usage efficiently.

### **4. Parallelism Strategies**

LLM training employs **various forms of parallelism**:

1. **Data Parallelism** – Replicates models across devices, aggregates gradients.
2. **Tensor Parallelism** – Splits model weights across devices (1D, 2D, 2.5D, 3D variants).
3. **Pipeline Parallelism** – Partitions model layers into stages, introduces micro-batching but suffers from **pipeline bubbles** and **memory imbalance**.
4. **Sequence Parallelism** – Splits inputs by sequence length for long-context training.
5. **Expert Parallelism (MoE)** – Trains sparse models by routing tokens to selected experts.

Additional methods include **auto-parallelism frameworks** (Alpa, GSPMD, FlexFlow) and **heterogeneous parallelism** (leveraging diverse hardware or model components).

### **5. Computation and Memory Optimizations**

- **Operator optimizations**: Efficient kernels (FlashAttention, fused ops) and reduced-precision formats (FP16/BF16/FP8).
- **Mixed-precision training**: Improves speed and reduces memory usage.
- **Memory optimizations**:

  - **Activation recomputation** (trade compute for memory).
  - **Redundancy reduction** (ZeRO, sharded optimizers).
  - **Memory defragmentation and offloading**.

### **6. Communication Optimizations**

- **Collective communication libraries** (NCCL, RCCL) are optimized for All-Reduce, All-Gather.
- Techniques include **communication scheduling**, **in-network aggregation**, and overlapping communication with computation.

### **7. Fault Tolerance**

- Training jobs run for **weeks or months**, making failures inevitable.
- Methods include:

  - **Failure analysis & anomaly detection** (detect slowdowns, stragglers).
  - **Checkpoint-based recovery** (periodic state saving).
  - **Checkpoint-free recovery** (e.g., redundant execution, partial replay).

### **8. Future Directions**

- **Optical computing and optical networks** to overcome digital circuit bottlenecks.
- **Co-design of hardware, networking, and algorithms** for optimal scaling.
- **Dynamic parallelism and adaptive scheduling** for heterogeneous environments.
- **Energy-efficient training** with power-aware scheduling.

### **9. Conclusion**

The paper provides a **comprehensive overview** of the infrastructure, parallelization, and optimization techniques for efficient LLM training. The authors highlight that achieving **scalability, efficiency, and reliability** requires **joint innovations in hardware, networking, storage, scheduling, and software frameworks**. Future progress may depend on **novel hardware (e.g., optical accelerators)** and **advanced auto-parallelization methods**.
